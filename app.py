import os
import streamlit as st
from dotenv import load_dotenv

# âœ… Load .env variables FIRST
load_dotenv()

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_openrouter import OpenRouterLLM  # âœ… Make sure this is after load_dotenv

# âœ… Setup OpenRouter LLM using environment key
llm = OpenRouterLLM(model="mistralai/mistral-7b-instruct")

# âœ… Load and index documents
loader = TextLoader("data/admissions.txt")
documents = loader.load()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)

# âœ… Streamlit UI
st.set_page_config(page_title="ðŸ¤– JSOM Chatbot â€“ Ask Me Anything")
st.title("ðŸ¤– JSOM Chatbot â€“ Ask Me Anything")
query = st.text_input("What would you like to know about JSOM?")

if query:
    with st.spinner("Thinking..."):
        result = qa.invoke(query)
        st.markdown(f"**Answer:** {result['result']}")

        with st.expander("ðŸ“„ Context Chunks Used"):
            for i, doc in enumerate(result["source_documents"]):
                st.markdown(f"**Chunk {i+1}:** {doc.page_content.strip()}")
