import os
import streamlit as st
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
#from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# Load environment variables
load_dotenv()

# Load FAISS vectorstore
vectorstore = FAISS.load_local("embeddings/faiss_index", embeddings=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"), allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

os.environ["OPENROUTER_API_KEY"] = st.secrets["OPENROUTER_API_KEY"]


llm = ChatOpenAI(
    openai_api_key=os.environ["OPENROUTER_API_KEY"],
    openai_api_base="https://openrouter.ai/api/v1",
    model_name="mistralai/mistral-7b-instruct"  # You can change this to another OpenRouter model
)

# Load local LLM (Mistral via Ollama)
#llm = Ollama(model="mistral")

# QA chain
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)

# UI
st.set_page_config(page_title="ðŸ¤– JSOM Chatbot â€“ Ask Me Anything")
st.title("ðŸ¤– JSOM Chatbot â€“ Ask Me Anything")
query = st.text_input("What would you like to know about JSOM?")

if query:
    with st.spinner("Thinking..."):
        result = qa.invoke(query)
        st.markdown(f"**Answer:** {result['result']}")
        
        with st.expander("ðŸ“„ Context Chunks Used"):
            for i, doc in enumerate(result["source_documents"]):
                st.markdown(f"**Chunk {i+1}:** {doc.page_content.strip()}")
